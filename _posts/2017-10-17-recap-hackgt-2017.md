---
layout: post
title: "Recap: HackGT 2017"
post-id: recap-hackgt-2017
thumbnail: photo-booth.jpeg
photoswipe: true
tags: 
  - Hackathons

demo:
  - src: face-demo.png
    maxres: 347x595
  - src: face-library.png
    maxres: 336x597

photobooth:
  - src: photo-booth.jpeg
    maxres: 1844x1240
---

This weekend I was at HackGT here in Atlanta. As much as I enjoy [traveling across the country to go to a hackathon](/blog/recap-hack-the-north-2017.html), they're just as fun when they're in your own backyard. 

<h4>The Project</h4>

{% include photoswipe.html images=page.demo %}

We built an app called Nametag AR that detects and remembers the names of people you meet. The app listens in the background for when people introduce themselves to you. When it detects an introduction (like **"Hi, I'm Jake"**), it saves the person's name and an image of their face. 

<!--break-->

Once you've met somebody, Nametag AR will remember them for you. Whenever you see them again in the future, the app will overlay their name near their face so you don't forget who they are.

We implemented the app using a few different core technologies. We used Apple's [**Speech**](https://developer.apple.com/documentation/speech) framework to continuously listen for the "Hi, I'm ___" keyword. Simultaneously, we use Apple's Vision framework to identify individual faces in the video feed and crop them in to standalone images. We combined these two inputs to built a database of known faces, with a name matched to an image. 

When the camera is pointed at a face, the app makes use of Microsoft Azure's Face API to compare the unknown face to the library of faces inside the app so far. If there's a match, the name is shown in augmented reality above the person's face.



