---
layout: post
title: "Recap: HackGT 2017"
post-id: recap-hackgt-2017
thumbnail: photo-booth.jpeg
photoswipe: true
tags: 
  - Hackathons

demo:
  - src: face-demo.png
    maxres: 347x595
  - src: face-library.png
    maxres: 336x597

friendship:
  - src: photo-booth.jpeg
    maxres: 1844x1240
---

This weekend I was at HackGT here in Atlanta. As much as I enjoy [traveling across the country to go to a hackathon](/blog/recap-hack-the-north-2017.html), they're just as fun when they're in your own backyard. 

<h4>The Project</h4>

{% include photoswipe.html images=page.demo %}

We built an app called Nametag AR that detects and remembers the names of people you meet. The app listens in the background for when people introduce themselves to you. When it detects an introduction (like *"Hi, I'm Jake"*), it saves the person's name and an image of their face. 

<!--break-->

Once you've met somebody, Nametag AR will remember them for you. Whenever you see them again in the future, the app will overlay their name near their face so you don't forget who they are.

We implemented the app using a few different core technologies. We used Apple's [Speech](https://developer.apple.com/documentation/speech) framework to continuously listen for the "Hi, I'm ___" keyword. Simultaneously, we use Apple's [Vision](https://developer.apple.com/documentation/vision) framework to identify individual faces in the video feed and crop them in to standalone images. We combined these two inputs to built a database of known faces, with a name matched to an image. 

When the camera is pointed at a face, the app makes use of Microsoft Azure's [Face API](https://azure.microsoft.com/en-us/services/cognitive-services/face/) to compare the unknown face to the library of faces inside the app so far. If there's a match, the name is shown in augmented reality above the person's face.

<h4>Pivoting to Azure</h4>

We were originally using the Python [openface](http://cmusatyalab.github.io/openface/) tool to convert faces into higher-dimentional vector representations. Theoretically, you can find the similarity of two faces by taking the euclidian distance of their vectors. In practice, though, we found this system very unreliable and noisy. The tool also suffered from long processing times, which hampered the augmented reality experience.

With about eight hours left in the hackathon, we pivoted away from openface and started using Microsoft Azure's Face API instead. We're really happy with this change, because it really increased the performance and the accuracy of our facial match system. It was pretty intimidating to make such a fundamental change so late in the game, though. I was impressed we pulled it off (we only got about half an hour of sleep that night).

<h4>Video Demo</h4>

{% include youtube.html url='https://www.youtube.com/embed/Trg_Z-Nh9Ws' %}

<h4>The Team</h4>

{% include photoswipe.html images=page.friendship %}

I was originally planning on working with a four-person team, but some registration woes whittled us down to just two people. I wasn't complaining, though -- I really enjoy working with [Nate](http://www.natethompson.io/). This was our third hackathon together. Most recently, we won 2nd place at HackCU in Boulder, CO for building [SiriQuery](https://devpost.com/software/siriquery). 

On a short tangent, I personally find working on large teams (4+ people) at hackathons especially difficult. When all four people have different skill sets, development environments, and interests, it's very difficult to come up with a project that gives everybody something to work on that's both interesting to them and well-scoped to their skill level. I think two or three people is the ideal size for a team that can perform really well together.